{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I cope with the flood of arXiv papers\n",
    "> hope, that it could help you as well\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [arXiv, research, preprints]\n",
    "- image: images/arxiv-sanity-small.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I don't \n",
    "\n",
    "OK, that is a joke -- I believe that I am dealing with the tons of papers rather fine. Here is how.\n",
    "\n",
    "\n",
    "# How and where to look?\n",
    "\n",
    "1. **Coarse-to-fine or Funnel**. I **check 100** paper titles + abstracts, I **skim through maybe 10**, I **read one or two**  papers carefully . \n",
    "\n",
    "2. \"**Features**\" I looks for:  - topic, theory/practice, figures quality, field crowdness, potential impact.\n",
    "\n",
    "3. **arXiv-sanity** as a main provider and the other sources.\n",
    "\n",
    "## Coarse-to-fine scheme\n",
    "\n",
    " I check 100 paper titles + abstracts, I skim through maybe 10, I read carefully one or two papers. \n",
    "Why? The most of papers are not relevant to me as a computer vision researcher. Some of the papers are bad. From those, which are good, the most important thing is their main message, not some details. And only little number of papers are worth reading -- for me. For you that would be different 1 or 2 papers out of 100, but likely not more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to ask yourself\n",
    "\n",
    "I will try to show what I am looking for and also a formulate for myself what is going on inside of my \"wet neural network\" when staring on arXiv-sanity page. I ask myself the following questions, and if the answer is \"yes\" on some of them, the paper is going to be downloaded.\n",
    "By \"read\" in this section I mean \"skim through abstract, conclusions, tables and figures to decide\". \n",
    "\n",
    "- **My areas? Yes -> read**. \n",
    "\n",
    "WxBS (image matching), 3d reconstruction, image retrieval, metric learning, RANSAC, CNN initialization.\n",
    "\n",
    "- **Opens a new (sub-)area of research? Yes -> read**. \n",
    "\n",
    "E.g. first papers on GAN, NERF, Transformers, Lottery ticket hypothesis. It does not matter if the area is not relevant for me, e.g. image-to-image translation. New area papers are always worth to read. \n",
    "\n",
    "- **From the crowded area?  Yes -> skip**.\n",
    "\n",
    "How many papers on the topic are published every day?  E.g., in 2014-2015 I have followed the research on semantic segmentation, object detection and GANs. Now I mostly skip all the papers related to GAN, segmenation and so on, because the improvements become incremental AND that areas are not mine. Ofc, from time to time I read some paper on GAN, but only if it comes by other channel - e.g. recommended to me by a colleague. \n",
    "\n",
    "Two previous points can be seen as an idf -- paper score normalization by average number of the papers on the topic.\n",
    "\n",
    "- **Dataset or large-scale benchmark paper? Yes -> read**, regargless of the topic. \n",
    "\n",
    "Why? It is useful to see how people gather data, clean the data, come up with a metrics and so on.\n",
    "\n",
    "- **Simple baseline? Yes -> read**, regadless of the area.\n",
    "\n",
    "- About **understanding some aspect of machine learning? Yes -> read if have time**\n",
    "\n",
    "E.g. padding, double descent, over-parametrization.\n",
    "\n",
    "- **Theory paper? Yes -> skip.**  Unless it touches very important topic for me. \n",
    "\n",
    "- **Relevant for me as a user? Yes -> read if have time**.\n",
    "\n",
    "E.g. new non-linearity, optimizer, etc. \n",
    "\n",
    "- **Am I a reviewer of that paper? If yes -> bad luck**.\n",
    "\n",
    "I have to really read this paper several times regardless of anything.\n",
    "\n",
    "\n",
    "In addition, I use some kind of \"paper gestalt\" -- does it looks as high quality work? Isn't the title over-keyworded and so on -- this kind of things are hard to verbalize. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use arXiv-sanity, not arXiv feed\n",
    "\n",
    "First, given the over-whelming popularity of this site, sometimes it is down. That is good - if it is down, then I do not check papers today. Nothing bad would happen is I skip paper reading today.\n",
    "\n",
    "\n",
    "Second, it shows first 8 pages thumbnails and the abstracts. This helps to make a more informed decision on whether to download the paper or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional (filtered) sources of papers\n",
    "\n",
    "1. Twitter feed. My twitter is very curated -- if someone tweets about the paper, it is going to be relevant\n",
    "\n",
    "2. ResearchGate \"You have a new citation of\". That is mostly the way how I find about the papers, which use [kornia](https://github.com/kornia/kornia), to be promoted on [kornia twitter](https://twitter.com/kornia_foss)\n",
    "\n",
    "3. Google Scholar recommendations. They are slow: appear a week after the ResearchGate shows me the paper. But -- Google Scholar recommendations cover the papers, which are not on arXiv. Thanks to university access, I am able to download the most of them, although not all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. Hope this helps :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
